---
description: downloading grib files, wave forecast wind forecast data
globs: *.py
---
# how to download grib files for wind and wave forecasts

## Wave Forecast Processing Rules
wave forecast is processed through /waves/v2/[station_id]
- endpoint in [wave_routes_v2.py](mdc:features/waves/routes/wave_routes_v2.py) 
- service is [wave_data_service_v2.py](mdc:features/waves/services/wave_data_service_v2.py)
- client to download files is [gfs_wave_client.py](mdc:features/waves/services/gfs_wave_client.py)
- [wave_routes.py](mdc:features/waves/routes/wave_routes.py) [wave_data_service.py](mdc:features/waves/services/wave_data_service.py) is DEPRECATED
- refer to [config.py](mdc:core/config.py) – keep this updated
   -- TO DO: this needs to be organized into wind, wave config
- use types everywhere [wave_types.py](mdc:features/waves/models/wave_types.py) e.g avoid passing around objects, and dicts.

## types 
- wave forecast types are in [wave_types.py](mdc:features/waves/models/wave_types.py)
- station [station_types.py](mdc:features/common/models/station_types.py), use `Station` everywhere, not objects

## url structure

### base url
```https://nomads.ncep.noaa.gov/cgi-bin/filter_gfswave.pl```

### complete url 
```https://nomads.ncep.noaa.gov/cgi-bin/filter_gfswave.pl?file=gfswave.t18z.atlocn.0p16.f000.grib2&lev_surface=on&var_DIRPW=on&var_HTSGW=on&var_PERPW=on```

## Data Structure

### GRIB Variables
- `swh`: Significant wave height [meters, ≥ 0]
- `perpw`: Peak wave period [seconds, ≥ 0]
- `dirpw`: Primary wave direction [degrees true, 0-360]

### Coordinate System
- Longitude: 0-360° system (convert from -180/180)
- Latitude: -90° to 90° (unchanged)
- Time: UTC timestamps from `valid_time` field

### Regional Boundaries
Atlantic Region:
- Longitude: 260° to 310° (convert from -100° to -50°)
- Latitude: 0° to 55°
- Grid Resolution: 0.167° (~18.5km)

Pacific Region:
- Longitude: 180° to 245°
- Latitude: 0° to 60°
- Grid Resolution: 0.167° (~18.5km)

## Processing Pipeline

1. **Data Loading**
   - Use Dask-enabled xarray for parallel processing
   - Chunk by time dimension for memory efficiency
   - Load only required variables (swh, perpw, dirpw)

```python
ds = xr.open_dataset(
    grib_file,
    engine="cfgrib",
    chunks={'time': -1},
    backend_kwargs={'indexpath': ''}
)
```

2. **Coordinate Processing**
   - Always use `valid_time` as time coordinate
   - Convert longitudes to 0-360° system
   - Use nearest-neighbor interpolation to find the nearest station against grib data

3. **Data Validation**
   - Validate ranges:
     - Heights ≥ 0
     - Periods ≥ 0
     - Directions 0-360°
   - Skip invalid points (don't interpolate)
   - Log but don't fail on individual point errors

4. **Time Handling**
   - All times in UTC
   - Sort forecasts chronologically
   - 3-hour intervals (0.125 days)
   - 168-hour forecast range (7 days)

## Caching Strategy

1. **GRIB File Cache**
   - Cache Location: `cache/gfs_wave/`
   - Naming: `{region}_gfs_{YYYYMMDD}_{HH}z_f{FFF}.grib2`
   - Expire after 4 hours (matches model run frequency)

2. **Dataset Cache**
   - Cache processed regional datasets in memory
   - Key format: `{region}_{YYYYMMDD}_{HH}`
   - Clear on new model run

3. **Forecast Cache**
   - Cache station forecasts
   - Key format: `gfs_wave_forecast:{station_id}`
   - Expire after 4 hours

## Error Handling

1. **Download Errors**
   - Retry with exponential backoff
   - Maximum 3 retries
   - Log failed downloads but continue processing

2. **Processing Errors**
   - Log invalid data points
   - Skip bad time points
   - Continue processing valid points
   - Return empty forecast only if no valid points

3. **Validation Errors**
   - Log out of range values
   - Skip invalid points
   - Include grid coordinates in error logs

## Performance Guidelines

1. **Memory Management**
   - Use Dask for large datasets
   - Process in chunks by time
   - Clear memory after processing
   - Close datasets when done

2. **Concurrency**
   - Download files concurrently (max 5)
   - Process regions independently
   - Use asyncio for I/O operations
   - Maintain connection pool